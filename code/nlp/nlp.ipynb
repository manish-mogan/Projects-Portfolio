{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef1c363b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install nltk wordcloud matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c73d950e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#import libraries\n",
        "\n",
        "import os\n",
        "import nltk\n",
        "import wordcloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ssl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deda563c",
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5824a8a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read the whole novel Great Expectations\n",
        "with open('data/great_expectations.txt', 'r', encoding='utf-8') as file:\n",
        "    filedata = file.read()\n",
        "\n",
        "# check the contents\n",
        "print(filedata[0:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2190710",
      "metadata": {},
      "outputs": [],
      "source": [
        "#read text using the Corpus Reader\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "#read the file into a corpus\n",
        "corpus = PlaintextCorpusReader(os.getcwd() + '/data/', 'great_expectations.txt')\n",
        "\n",
        "#print the beginning of the corpus\n",
        "print(corpus.raw()[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6183c767",
      "metadata": {},
      "outputs": [],
      "source": [
        "#extract the file ids\n",
        "file_ids = corpus.fileids()\n",
        "print('Files in this corpus:', file_ids)\n",
        "\n",
        "#extract the paragraphs\n",
        "paragraphs = corpus.paras('great_expectations.txt')\n",
        "print('No. of paragraphs:', len(paragraphs))\n",
        "\n",
        "#extract the sentences\n",
        "sentences = corpus.sents('great_expectations.txt')\n",
        "print('No. of sentences:', len(sentences))\n",
        "\n",
        "words = corpus.words()\n",
        "print('No. of words:', len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06eeeaa5",
      "metadata": {},
      "outputs": [],
      "source": [
        "#extract tokens\n",
        "token_list = nltk.word_tokenize(corpus.raw('great_expectations.txt'))\n",
        "print('No. of tokens:', len(token_list))\n",
        "print('First 10 tokens:', token_list[0:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a7c86ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove punctuation marks\n",
        "\n",
        "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
        "\n",
        "print('Total tokens without punctuation:', len(token_list2))\n",
        "print('Some tokens:', token_list2[0:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f0ddda0",
      "metadata": {},
      "outputs": [],
      "source": [
        "#convert to lower case\n",
        "token_list3 = [token.lower() for token in token_list2]\n",
        "print('Some tokens in lower case:', token_list3[0:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "580b4216",
      "metadata": {},
      "outputs": [],
      "source": [
        "#eliminate words of length 2 or less\n",
        "token_list4 = [word for word in token_list3 if len(word) > 2]\n",
        "print('Some tokens after removing short words:', token_list4[0:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdae4d15",
      "metadata": {},
      "outputs": [],
      "source": [
        "#find the frequency distribution of the words\n",
        "freq_dist = nltk.FreqDist(token_list4)\n",
        "print('Most common words:', freq_dist.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10621ca4",
      "metadata": {},
      "outputs": [],
      "source": [
        "#get stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print('Some stop words:', list(stop_words)[0:20])\n",
        "\n",
        "\n",
        "#remove stop words from the token list\n",
        "token_list5 = list(filter(lambda word: word not in stop_words, token_list4))\n",
        "print('Some tokens after removing stop words:', token_list5[0:20])\n",
        "print('Total words after removing stop words:', len(token_list5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3681157",
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualization of the cumulative distribution of the top 50 words\n",
        "plt.figure(figsize=(12,6))\n",
        "freq_dist.plot(50, cumulative=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6622f56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# add to the stopwords list\n",
        "new_stop_words = ['miss', 'mr', 'said', 'one', 'upon', 'come', 'little', 'know', 'away', 'good', 'great']\n",
        "stop_words.update(new_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e8cd2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove stopwords\n",
        "token_list6 = list(filter(lambda word: word not in stop_words, token_list5))\n",
        "print('Some tokens after removing additional stop words:', token_list6[0:20])\n",
        "print('Total words after removing additional stop words:', len(token_list6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f012d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "#represent the novel as a visual\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "#generate the wordcloud data\n",
        "wordcloud = WordCloud(stopwords = stop_words, max_words=100, background_color=\"white\").generate(' '.join(token_list6))\n",
        "\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6853ec60",
      "metadata": {},
      "outputs": [],
      "source": [
        "###### Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "#use the wordnet library to map their lemmatized form\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b540e65",
      "metadata": {},
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "token_list7 = [lemmatizer.lemmatize(word) for word in token_list6]\n",
        "\n",
        "print('Total tokens after lemmatization:', len(token_list7))\n",
        "print('Some tokens after lemmatization:', token_list7[0:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c108eac2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5140af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# find bigrams\n",
        "bigrams = ngrams(token_list7, 2)\n",
        "bigram_freq = Counter(bigrams)\n",
        "\n",
        "print('Most common bigrams:', bigram_freq.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78127345",
      "metadata": {},
      "outputs": [],
      "source": [
        "#find trigrams and print the most common 10\n",
        "trigrams = ngrams(token_list7, 3)\n",
        "trigram_freq = Counter(trigrams)\n",
        "print('Most common trigrams:', trigram_freq.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0460596c",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Parts of Speech Tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "pos_tags = nltk.pos_tag(token_list6)\n",
        "print('Some parts of speech tags:', pos_tags[0:20])\n",
        "\n",
        "#tag and print the first 20 tokens\n",
        "nltk.pos_tag(token_list6[0:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "624a8989",
      "metadata": {},
      "outputs": [],
      "source": [
        "#sentiment analysis\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "filedata = filedata.replace('\\n', ' ')\n",
        "sentences = sent_tokenize(filedata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b21ef6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(sentences[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47fb804a",
      "metadata": {},
      "outputs": [],
      "source": [
        "#create a dataframe with sentences\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(sentences, columns=['sentence'])\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a7d71b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2f4fa0",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b430c91f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1992e400",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c91d1b2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Sentiment Analysis using VADER\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d8c8b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['compound'] = df['sentence'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "df['neg'] = [analyzer.polarity_scores(x)['neg'] for x in df['sentence']]\n",
        "df['neu'] = [analyzer.polarity_scores(x)['neu'] for x in df['sentence']]\n",
        "df['pos'] = [analyzer.polarity_scores(x)['pos'] for x in df['sentence']]\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dc9a566",
      "metadata": {},
      "outputs": [],
      "source": [
        "#get the number of positive, neutral, and negative scores\n",
        "pos_sent = df.loc[df['compound'] > 0]\n",
        "neg_sent = df.loc[df['compound'] < 0]\n",
        "neu_sent = df.loc[df['compound'] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3a75829",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "print(len(pos_sent))\n",
        "print(len(neg_sent))\n",
        "print(len(neu_sent))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6eba13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize distribution of sentiment scores\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df['compound'], bins=30, kde=True)\n",
        "plt.title('Distribution of Compound Sentiment Scores')\n",
        "plt.xlabel('Compound Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667578ba",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
